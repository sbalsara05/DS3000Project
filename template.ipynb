{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the template for DS3000 Final data analysis project. Once you finish, please remove all my instructions. You do not need to exactly follow the structure in the template but please make sure you have all the components. Write your report in paragraphs. Only use bullet points when list something (eg: functions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating Palworld Through Data Analysis\n",
    "#### Team 11\n",
    "- Ansh Aggarwal\n",
    "- Sohum Balsara\n",
    "- Bear Smith"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project we study Palworld, a survival and monster-taming video game in which players collect characters called Pals to battle each other and help build their bases. Each Pal has a different set of skills and qualifications, a combination of such features as their element type, their HP in battle, and their \"work suitability\" for various tasks like farming or electricity production. We want to learn about how those factors combine and influence each other, and what relationships that may not be stated outwardly can be found between them. This may help players better understand their game and be more successful at it, or at least increase their knowledge of the logic behind Palworld.\n",
    "\n",
    "Our primary research questions are:\n",
    "- How are a Pal's work suitability scores correlated to its other statistics, such as its element type or battle skills?\n",
    "- What relationship does Pal rarity have with their other statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "\n",
    "### Data Source\n",
    "\n",
    "- All data was taken from [palworld.gg](\"https://palworld.gg\").\n",
    "- We scraped each Pal's name, ID number, rarity, element(s), work suitability type(s) and level(s), HP, and defense scores.\n",
    "- To make the data frame more easily filtered, the Pals' elements were listed using multiple boolean columns, each representing one element.\n",
    "- For that same reason, their work suitabilities were also listed by column, but instead of True or False, each value was the Pal's level in that suitability or Nan if they did not have it. That way, dropna can be used to filter the data frame by element type.\n",
    "- Finally, the Pals' HP and defense stats were converted to numbers in the data frame, so that mathematical operations can be done on those columns.\n",
    "\n",
    "### Webscraping and cleaning functions overview\n",
    "- `scrape_pal_ids()`\n",
    "    - Creates a data frame with all the Pals' names as its index and one column of data containing their IDs.\n",
    "- `scrape_pal_rarity()`\n",
    "    - Adds a column containing the rarity of each Pal to the given data frame.\n",
    "- `scrape_pal_elements()`\n",
    "    - Adds the element or elements of each Pal to the given data frame using one-hot columns.\n",
    "- `scrape_pal_work()`\n",
    "    - Adds the work suitability or suitabilities of each Pal to the given data frame, along with its level.\n",
    "- `scrape_pal_hp()`\n",
    "    - Adds a column containing the HP of each Pal to the given data frame.\n",
    "- `scrape_pal_defense()`\n",
    "    - Adds a column containing the defense score of each Pal to the given data frame.\n",
    "\n",
    "### Data overview\n",
    "\n",
    "# TODO - IN PROGRESS\n",
    "- You can see a couple rows of our data below.\n",
    "- We store the Pals' rarity, their element or elements, their work suitabilities and what levels they are, \n",
    "- Discuss if there is any potential problems about the data (eg: missing values, any features that you did not collect but may be important, any other concerns)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "pal_frame = pd.read_excel(\"pal_frame_for_project.xlsx\")\n",
    "# Reset the index back to the Pal names after reading in the spreadsheet file.\n",
    "pal_frame.set_index(\"Unnamed: 0\", inplace = True)\n",
    "pal_frame.rename_axis(None, axis = 0, inplace = True)\n",
    "pal_frame.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# list all the functions you have for webscraping and cleaning. Make sure write full \n",
    "# docstrings for each function\n",
    "\n",
    "def scrape_pal_ids(url = \"https://palworld.gg/pals\"):\n",
    "    \"\"\"\n",
    "    Scrape Pal IDs from the given URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the Palworld database page, assumed to be the current link unless otherwise specified.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A dataFrame of Pal names and IDs.\n",
    "    \"\"\"\n",
    "    response = requests.get(url).text\n",
    "    soup = BeautifulSoup(response)\n",
    "\n",
    "    # Find all Pal ID elements\n",
    "    pal_dict = {}\n",
    "    for pal in soup.find_all(\"div\", class_ = \"pal\"):\n",
    "\n",
    "        # Remove empty Pal entries.\n",
    "        if pal.attrs[\"style\"] == \"display:none;\":\n",
    "            continue\n",
    "\n",
    "        # Get the ID and name of the Pal and add them to the dictionary.\n",
    "        pal_id_element = pal.find('span', class_='index').text.strip()\n",
    "        pal_dict[pal.find(\"div\", class_ = \"name\").next_element.strip()] = pal_id_element\n",
    "\n",
    "    # Convert the dictionary to a DataFrame before returning it.\n",
    "    return pd.DataFrame().from_dict(pal_dict, orient = \"index\", columns = [\"ID\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def scrape_pal_rarity(pal_df, url = \"https://palworld.gg/pals\"):\n",
    "    \"\"\"\n",
    "    Scrape the rarity of each Pal listed in the database.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the database page, assumed to be the current link unless otherwise specified.\n",
    "        pal_df (DataFrame): The DataFrame of Pal data to update.\n",
    "\n",
    "    Returns:\n",
    "        pal_df (DataFrame): The DataFrame given, with the rarity of each Pal added under a new column.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(requests.get(url).text)\n",
    "\n",
    "    # Find all Pals, then find each of their rarities.\n",
    "    pal_tag = soup.find_all(\"div\", class_ = \"pal\")\n",
    "    for pal in pal_tag:\n",
    "\n",
    "        # Remove empty Pal entries.\n",
    "        if pal.attrs[\"style\"] == \"display:none;\":\n",
    "            continue\n",
    "\n",
    "        # The \"name\" class is used twice in each Pal entry, first for their name, then for their rarity.\n",
    "        name_class = pal.find_all(\"div\", class_ = \"name\")\n",
    "        # (Using next_element here rather than .text to avoid also getting the text from the nested children.)\n",
    "        # Add the rarity of the Pal to the DataFrame under its name.\n",
    "        pal_df.loc[name_class[0].next_element.strip(), \"Rarity\"] = name_class[1].next_element\n",
    "\n",
    "    return pal_df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def scrape_pal_elements(pal_df, url = \"https://palworld.gg/pals\"):\n",
    "    \"\"\"\n",
    "    Scrape the element or elements of each Pal listed in the database.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the database page, assumed to be the current link unless otherwise specified.\n",
    "        pal_df (DataFrame): The DataFrame of Pal data to update.\n",
    "\n",
    "    Returns:\n",
    "        pal_df (DataFrame): The DataFrame given, with a dummy variable added for each element,\n",
    "        True meaning that a Pal does belong to them and False for not.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(requests.get(url).text)\n",
    "\n",
    "    # Find each Pal entry.\n",
    "    pal_tag = soup.find_all(\"div\", class_ = \"pal\")\n",
    "    for pal in pal_tag:\n",
    "        # Remove empty Pal entries.\n",
    "        if pal.attrs[\"style\"] == \"display:none;\":\n",
    "            continue\n",
    "\n",
    "        # Load the individual entry page for the Pal.\n",
    "        pal_page = requests.get(\"https://palworld.gg\" + pal.a.attrs[\"href\"]).text\n",
    "        pal_soup = BeautifulSoup(pal_page)\n",
    "\n",
    "        # Scrape the Pal's name.\n",
    "        pal_name = pal_soup.find(\"h1\", class_ = \"name\").text.strip()\n",
    "\n",
    "        # Initialize all the element column values for this Pal to False, as none have been found yet.\n",
    "        pal_df.loc[pal_name, [\"Earth\", \"Fire\", \"Dragon\", \"Dark\", \"Electricity\", \"Water\", \"Ice\", \"Leaf\", \"Normal\"]] = False\n",
    "\n",
    "        # Scrape the elements of the Pal.\n",
    "        pal_elems_tags = pal_soup.find(\"div\", class_ = \"elements\").find_all(\"div\", class_ = \"name\")\n",
    "        for tag in pal_elems_tags:\n",
    "            # For each found element, change the Pal's value in the corresponding column to True.\n",
    "            pal_df.loc[pal_name, tag.text] = True\n",
    "        \n",
    "    return pal_df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def scrape_pal_work(pal_df, url = \"https://palworld.gg/pals\"):\n",
    "    \"\"\"\n",
    "    Scrape the work suitabilities of each Pal listed in the database, including the level of each work type.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the database page, assumed to be the current link unless otherwise specified.\n",
    "        pal_df (DataFrame): The DataFrame of Pal data to update.\n",
    "\n",
    "    Returns:\n",
    "        pal_df (DataFrame): The DataFrame given, with 13 new columns added: 12 representing the work suitabilities\n",
    "        of Pals, each filled with NaN or the Pal's skill level for that task, and one column counting the number of suitabilities\n",
    "        the Pal has in total.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(requests.get(url).text)\n",
    "\n",
    "    # Find each Pal entry.\n",
    "    pal_tag = soup.find_all(\"div\", class_ = \"pal\")\n",
    "    for pal in pal_tag:\n",
    "        # Remove empty Pal entries.\n",
    "        if pal.attrs[\"style\"] == \"display:none;\":\n",
    "            continue\n",
    "\n",
    "        # Load the individual entry page for the Pal.\n",
    "        pal_page = requests.get(\"https://palworld.gg\" + pal.a.attrs[\"href\"]).text\n",
    "        pal_soup = BeautifulSoup(pal_page)\n",
    "\n",
    "        # Scrape the name of the Pal.\n",
    "        pal_name = pal_soup.find(\"h1\", class_ = \"name\").text.strip()\n",
    "\n",
    "        # Scrape the suitabilities of the Pal, and for each one that is found add its level to the column with the suitability's name.\n",
    "        pal_work_tags = pal_soup.find(\"div\", class_ = \"works\").find_all(\"div\", class_ = \"active item\")\n",
    "        for tag in pal_work_tags:\n",
    "            pal_df.loc[pal_name, tag.find(\"div\", class_ = \"name\").text] = tag.find(\"span\", class_ = \"value\").text\n",
    "        # Add the pal's total number of suitabilities as a new column.\n",
    "        pal_df.loc[pal_name, \"Number of work suitabilities\"] = len(pal_work_tags)\n",
    "        \n",
    "    return pal_df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def scrape_pal_hp(pal_df, url = \"https://palworld.gg/pals\"):\n",
    "    \"\"\"\n",
    "    Scrape the HP of each Pal listed in the database.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the database page, assumed to be the current link unless otherwise specified.\n",
    "        pal_df (DataFrame): The DataFrame of Pal data to update.\n",
    "\n",
    "    Returns:\n",
    "        pal_df (DataFrame): The DataFrame given, with a new column representing the Pals' HPs.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(requests.get(url).text)\n",
    "\n",
    "    # Find each Pal entry.\n",
    "    pal_tag = soup.find_all(\"div\", class_ = \"pal\")\n",
    "    for pal in pal_tag:\n",
    "        # Remove empty Pal entries.\n",
    "        if pal.attrs[\"style\"] == \"display:none;\":\n",
    "            continue\n",
    "\n",
    "        # Load the individual entry page for the Pal.\n",
    "        pal_page = requests.get(\"https://palworld.gg\" + pal.a.attrs[\"href\"]).text\n",
    "        pal_soup = BeautifulSoup(pal_page)\n",
    "\n",
    "        # Scrape the name of the Pal.\n",
    "        pal_name = pal_soup.find(\"h1\", class_ = \"name\").text.strip()\n",
    "\n",
    "        # Scrape the HP of the pal (the first entry in the stats section) and add it to the HP column.\n",
    "        pal_hp = int(pal_soup.find(\"div\", class_ = \"stats\").find_all(\"div\", class_ = \"value\")[0].text)\n",
    "        pal_df.loc[pal_name, \"HP\"] = pal_hp\n",
    "        \n",
    "    return pal_df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def scrape_pal_defense(pal_df, url = \"https://palworld.gg/pals\"):\n",
    "    \"\"\"\n",
    "    Scrape the defense score of each Pal listed in the database.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the database page, assumed to be the current link unless otherwise specified.\n",
    "        pal_df (DataFrame): The DataFrame of Pal data to update.\n",
    "\n",
    "    Returns:\n",
    "        pal_df (DataFrame): The DataFrame given, with a new column representing the Pals' defense scores.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(requests.get(url).text)\n",
    "\n",
    "    # Find each Pal entry.\n",
    "    pal_tag = soup.find_all(\"div\", class_ = \"pal\")\n",
    "    for pal in pal_tag:\n",
    "        # Remove empty Pal entries.\n",
    "        if pal.attrs[\"style\"] == \"display:none;\":\n",
    "            continue\n",
    "\n",
    "        # Load the individual entry page for the Pal.\n",
    "        pal_page = requests.get(\"https://palworld.gg\" + pal.a.attrs[\"href\"]).text\n",
    "        pal_soup = BeautifulSoup(pal_page)\n",
    "\n",
    "        # Scrape the name of the Pal.\n",
    "        pal_name = pal_soup.find(\"h1\", class_ = \"name\").text.strip()\n",
    "\n",
    "        # Scrape the defense of the pal (the second entry in the stats section) and add it to the defense column.\n",
    "        pal_hp = int(pal_soup.find(\"div\", class_ = \"stats\").find_all(\"div\", class_ = \"value\")[1].text)\n",
    "        pal_df.loc[pal_name, \"Defense\"] = pal_hp\n",
    "        \n",
    "    return pal_df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pal_df = scrape_pal_defense(scrape_pal_hp(scrape_pal_work(scrape_pal_elements(scrape_pal_rarity(scrape_pal_ids())))))\n",
    "pal_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "### Visualization functions overview\n",
    "List all the functions you have written for visualization. For each one, write one sentence to describe it. \n",
    "- `make_hist()`\n",
    "    - Generate a histogram with given data and feature\n",
    " \n",
    "### Visualization results\n",
    "- Present 3-4 data visualizations.\n",
    "- For each visualization, you need to include title, xlabel, ylabel, legend (if necessary)\n",
    "- For each visualization, explain why you make this data visualization (how it related to your research question) and explain what you have learned from this visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# list all the functions you have for visualization. Make sure write full \n",
    "# docstrings for each function\n",
    "def make_hist(df, y_feat):\n",
    "\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write the code to run functions to get each data visualization in separate code chunks. \n",
    "# Interpret the figures. "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualization 2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write the code to run functions to get each data visualization in separate code chunks. \n",
    "# Interpret the figures. "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualization 3"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write the code to run functions to get each data visualization in separate code chunks. \n",
    "# Interpret the figures. "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### Modeling functions overview\n",
    "List all the functions you have written for modeling. For each one, write one sentence to describe it. \n",
    "- `fit_linear()`\n",
    "    - fit a linear model to the data and output the r2, slope and intercept\n",
    "\n",
    "### Model results\n",
    "\n",
    "- Present 2-3 models for the analysis.\n",
    "- Explain any pre-processing steps you have done (eg: scaling, polynomial, dummy features)\n",
    "- For each model, explain why you think this model is suitable and what metrics you want to use to evaluate the model\n",
    "    - If it is a classification model, you need to present the confusion matrix, calculate the accuracy, sensitivity and specificity with cross-validation\n",
    "    - If it is a regression model, you need to present the r2 and MSE with cross-validation\n",
    "    - If it is a linear regression model/multiple linear regression model, you need to interpret the meaning of the coefficient with the full data\n",
    "    - If it is a decision tree model, you need to plot the tree with the full data\n",
    "    - If it is a random forest model, you need to present the feature importance plot with the full data\n",
    "    - If it is a PCA, you need to explain how to select the number of components and interpret the key features in the first two components\n",
    "    - If it is a clustering, you need explain how to select the number of clustering and summarize the clustering. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# list all the functions you have for modeling. Make sure write full \n",
    "# docstrings for each function\n",
    "def fit_linear(df, y_feat, x_feat):\n",
    "    \"\"\"\n",
    "    Fit a linear model to the data and output the r2, slope and intercept.\n",
    "   Args:\n",
    "        df (DataFrame): The DataFrame of Pal data to use.\n",
    "        y_feat (str): The name of the feature to use as the dependent variable.\n",
    "        x_feat (str): The name of the feature to use as the independent variable.\n",
    "    Returns:\n",
    "        r2 (float): The r-squared value of the fitted model.\n",
    "        slope (float): The slope of the fitted model.\n",
    "        intercept (float): The intercept of the fitted model.\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import r2_score\n",
    "    import numpy as np\n",
    "    # Drop rows with NaN values in the specified columns.\n",
    "    clean_df = df[[y_feat, x_feat]].dropna()\n",
    "    X = clean_df[[x_feat]].to_numpy().reshape(-1, 1)\n",
    "    y = clean_df[y_feat].to_numpy()\n",
    "    # Split the data into training and testing sets.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    # Fit the linear regression model.\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    # Make predictions on the test set.\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Calculate the r-squared value, slope, and intercept.\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    slope = model.coef_[0]\n",
    "    intercept = model.intercept_\n",
    "    return r2, slope, intercept"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create Element Type column from one-hot element columns\n",
    "element_cols = [\"Earth\", \"Fire\", \"Dragon\", \"Dark\", \"Electricity\", \"Water\", \"Ice\", \"Leaf\", \"Normal\"]\n",
    "\n",
    "\n",
    "def get_element_type(row):\n",
    "    for col in element_cols:\n",
    "        if col in row.index and row[col] > 0:\n",
    "            return col\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "pal_frame[\"Element Type\"] = pal_frame.apply(get_element_type, axis=1)\n",
    "\n",
    "# Define fixed colors for each element\n",
    "element_colors = {\n",
    "    \"Earth\": \"#8B4513\",  # SaddleBrown\n",
    "    \"Fire\": \"#FF4500\",  # OrangeRed\n",
    "    \"Dragon\": \"#800080\",  # Purple\n",
    "    \"Dark\": \"#2F4F4F\",  # DarkSlateGray\n",
    "    \"Electricity\": \"#FFD700\",  # Gold\n",
    "    \"Water\": \"#1E90FF\",  # DodgerBlue\n",
    "    \"Ice\": \"#00CED1\",  # DarkTurquoise\n",
    "    \"Leaf\": \"#228B22\",  # ForestGreen\n",
    "    \"Normal\": \"#A9A9A9\",  # DarkGray\n",
    "    \"Unknown\": \"#808080\"  # Gray\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# visuals for model 1\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "# --- Detect work-type columns ---\n",
    "work_list = ['Handiwork', 'Mining',\n",
    "       'Transporting', 'Deforesting',\n",
    "       'Kindling', 'Gathering', 'Generating Electricity', 'Watering',\n",
    "       'Cooling', 'Farming', 'Medicine Production', 'Planting']\n",
    "work_columns = pal_frame[work_list]\n",
    "\n",
    "\n",
    "def build_features(df, work_cols):\n",
    "    out = df.copy()\n",
    "    for idx in range(len(work_cols)):\n",
    "        pal = work_cols.iloc[idx, :]\n",
    "        pal = pal.dropna()\n",
    "        out.loc[pal.name, \"Highest Work Suitability Level\"] = pal.sort_values(ascending = False).iloc[0]\n",
    "        out.loc[pal.name, \"Work Types\"] = \", \".join(pal.index)\n",
    "        out.loc[pal.name, \"Work Skill Count\"] = len(pal.index)\n",
    "\n",
    "    return out\n",
    "\n",
    "# --- Prepare data ---\n",
    "pal_frame_vis = build_features(pal_frame, work_columns)\n",
    "\n",
    "# --- 1. Scatter Plot ---\n",
    "fig1 = px.scatter(\n",
    "    pal_frame_vis.sort_values(by = \"Highest Work Suitability Level\", ascending = True),\n",
    "    x=\"Highest Work Suitability Level\",\n",
    "    y=\"HP\",\n",
    "    color=\"Element Type\",\n",
    "    hover_data={\"ID\":True, \"Rarity\":True, \"Work Types\":True},\n",
    "    title=\"Work Suitability vs HP by Element Type\"\n",
    ")\n",
    "fig1.show()\n",
    "\n",
    "# --- 2. Grouped Bar Chart ---\n",
    "melted = pal_frame_vis.melt(\n",
    "    id_vars=[\"Element Type\"],\n",
    "    value_vars=work_columns,\n",
    "    var_name=\"Work Type\",\n",
    "    value_name=\"Level\"\n",
    ")\n",
    "melted[\"Level\"] = pd.to_numeric(melted[\"Level\"], errors=\"coerce\")\n",
    "melted = melted.dropna(subset=[\"Level\"])\n",
    "\n",
    "agg_df = (melted.groupby([\"Element Type\", \"Work Type\"], as_index=False, observed=True)\n",
    "          .agg(Level=(\"Level\", \"mean\")))\n",
    "\n",
    "fig2 = px.bar(\n",
    "    agg_df,\n",
    "    x=\"Element Type\", y=\"Level\", color=\"Work Type\",\n",
    "    barmode=\"group\",\n",
    "    title=\"Average Work Suitability Level by Element Type & Work Type\"\n",
    ")\n",
    "fig2.show()\n",
    "\n",
    "\n",
    "# --- 3. Heatmap ---\n",
    "heat_df = pal_frame_vis.groupby([\"Rarity\",\"Work Skill Count\"]).size().reset_index(name=\"Count\")\n",
    "fig3 = px.density_heatmap(\n",
    "    heat_df,\n",
    "    x=\"Work Skill Count\", y=\"Rarity\", z=\"Count\", color_continuous_scale=\"Viridis\",\n",
    "    title=\"Rarity vs. Number of Work Suitabilities (Frequency)\"\n",
    ")\n",
    "fig3.show()\n",
    "\n",
    "# --- 4. Boxplots ---\n",
    "fig4 = px.box(pal_frame_vis, x=\"Rarity\", y=\"HP\", color=\"Element Type\",\n",
    "              title=\"HP Distribution by Rarity\")\n",
    "fig4.show()\n",
    "fig5 = px.box(pal_frame_vis, x=\"Rarity\", y=\"Defense\", color=\"Element Type\",\n",
    "              title=\"Defense Distribution by Rarity\")\n",
    "fig5.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Write the code to run functions to fit each model in separate code chunks.\n",
    "r2, slope, intercept = fit_linear(pal_frame_vis, \"HP\", \"Highest Work Suitability Level\")\n",
    "print(f\"Model 1: R2 = {r2}, Slope = {slope}, Intercept = {intercept}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write the code to run functions to fit each model in separate code chunks. \n",
    "# Interpret the model results.\n",
    "\n",
    "# Replot the scatter plot with the fitted line\n",
    "fig1 = px.scatter(\n",
    "    pal_frame_vis.sort_values(by = \"Highest Work Suitability Level\", ascending = True),\n",
    "    x=\"Highest Work Suitability Level\",\n",
    "    y=\"HP\",\n",
    "    color=\"Element Type\",\n",
    "    hover_data={\"ID\":True, \"Rarity\":True, \"Work Types\":True},\n",
    "    title=\"Work Suitability vs HP by Element Type\"\n",
    ")\n",
    "fig1.add_scatter(\n",
    "    x=pal_frame_vis[\"Highest Work Suitability Level\"],\n",
    "    y=intercept + slope * pal_frame_vis[\"Highest Work Suitability Level\"],\n",
    "    mode=\"lines\",\n",
    "    name=\"Fitted Line\",\n",
    "    line=dict(color=\"red\", width=2)\n",
    ")\n",
    "fig1.show()\n",
    "#Tthe fitted line shows a positive correlation between work suitability level and HP, indicating that Pals with higher work suitability levels tend to have higher HP. The slope of the line suggests that for each unit increase in work suitability level, the HP increases by approximately `slope` units. The R2 value indicates how well the model explains the variance in HP based on work suitability level."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write the code to run functions to fit each model in separate code chunks. \n",
    "# Interpret the model results.\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write the code to run functions to fit each model in separate code chunks. \n",
    "# Interpret the model results. "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "- One or two paragraphs to summarize your findings in the modeling sections and do the models answer your research question?\n",
    "- Any other potential thing you can do with the analysis (eg: include more features, get more data, try some other models etc.)\n",
    "- List the contribution for each group member.\n",
    "\n",
    "### Contributions\n",
    "- Sohum came up with the majority of our research questions and visualization ideas.\n",
    "- Bear wrote the web scraping functions.\n",
    "- Ansh wrote the data visualizations.\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
